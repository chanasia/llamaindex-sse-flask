{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHk8aGK2j4VH",
        "outputId": "1d5dfab8-8a22-466d-afed-be6358cf98ee"
      },
      "outputs": [],
      "source": [
        "! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.24 llama-index==0.9.19 pyngrok flask flask_cors gevent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Ht4avYkN8q"
      },
      "outputs": [],
      "source": [
        "model_url = \"https://huggingface.co/openthaigpt/openthaigpt-1.0.0-beta-13b-chat-gguf/resolve/main/ggml-model-q4_0.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SayUPgyWkTQN"
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHx9Nk5LkaS1",
        "outputId": "a6dcdd53-7ba3-4f42-e354-c6fd58fff21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading url https://huggingface.co/openthaigpt/openthaigpt-1.0.0-beta-13b-chat-gguf/resolve/main/ggml-model-q4_0.gguf to path /tmp/llama_index/models/ggml-model-q4_0.gguf\n",
            "total size (MB): 7430.46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7087it [00:25, 278.48it/s]                          \n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    model_url=model_url,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 43},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W8NC4Mxy2e9"
      },
      "outputs": [],
      "source": [
        "def generate_stream_sse(instruction,\n",
        "                    input=None,\n",
        "                    temperature=0.3,\n",
        "                    top_p=0.75,\n",
        "                    top_k=40,\n",
        "                    beams=1,\n",
        "                    max_tokens=256,\n",
        "                    frequency_penalty=0.3,\n",
        "                    is_streaming=False):\n",
        "\n",
        "  def generate_chat():\n",
        "    #Prompt formats\n",
        "    if input is None:\n",
        "      prompt = f\"\"\"### Instruction:\n",
        "  {instruction}\n",
        "\n",
        "  ### Response:\n",
        "  \"\"\"\n",
        "    else:\n",
        "      prompt = f\"\"\"### Instruction:\n",
        "  {instruction}\n",
        "\n",
        "  ### Input:\n",
        "  {input}\n",
        "\n",
        "  ### Response:\n",
        "  \"\"\"\n",
        "\n",
        "    payload = {\n",
        "        # \"prompt\": prompt,\n",
        "        \"stream\": is_streaming,\n",
        "        \"temperature\": 0 if beams>1 else temperature ,\n",
        "        \"top_p\": 1 if beams>1 else top_p,\n",
        "        \"top_k\": -1 if beams>1 else top_k,\n",
        "        \"use_beam_search\": beams>1,\n",
        "        \"n\": beams,\n",
        "        \"stop\":'<|endoftext|>',\n",
        "        \"frequency_penalty\": frequency_penalty,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        }\n",
        "\n",
        "    for response in llm.stream_complete(prompt, **payload):\n",
        "      yield f\"data: {response.raw['choices'][0]['text']}\\n\\n\"\n",
        "  return generate_chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpZAoHJE8yOi",
        "outputId": "c04e63e7-685c-4b00-fa22-e263fa71af5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your token: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "from pyngrok import ngrok, conf\n",
        "import nest_asyncio\n",
        "\n",
        "print(\"Enter your token: \", end=\"\")\n",
        "conf.get_default().auth_token = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "n4aeQFV9mq-E",
        "outputId": "e73ff7a6-c503-49c7-ee9e-14f652d370b6"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, Response, request, abort, jsonify\n",
        "import time\n",
        "\n",
        "from flask_cors import CORS\n",
        "from gevent.pywsgi import WSGIServer\n",
        "import warnings\n",
        "import json\n",
        "import copy\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"specific warning message\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "app.config['JSON_AS_ASCII'] = False\n",
        "\n",
        "@app.route(\"/completions\", methods=[\"POST\"])\n",
        "def completions():\n",
        "  datas = request.json\n",
        "  if not 'instruction' in datas: abort(404)\n",
        "  if not 'input' in datas: input = None\n",
        "  else: input = datas['input']\n",
        "  instruction = datas['instruction']\n",
        "\n",
        "  resp = Response(\n",
        "        generate_stream_sse(instruction, input),\n",
        "        mimetype='text/event-stream'\n",
        "    )\n",
        "  resp.headers['X-Accel-Buffering'] = 'no'\n",
        "  resp.headers['Cache-Control'] = 'no-cache'\n",
        "  return resp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  ngrok_tunnel = ngrok.connect(5000)\n",
        "  nest_asyncio.apply()\n",
        "  print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "  http_server = WSGIServer(('0.0.0.0', 5000), app)\n",
        "  http_server.serve_forever()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hFkouDYe7Ihe"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
